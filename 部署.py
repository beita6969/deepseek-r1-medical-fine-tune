# -*- coding: utf-8 -*-
"""部署.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oIbEXurMDLKi62dzdBccFsnXRVFnVPai
"""

!pip install streamlit torch transformers

!pip install pyngrok

# Commented out IPython magic to ensure Python compatibility.
# %%writefile medical_qa_app.py
# import os
# import torch
# from transformers import AutoTokenizer, AutoModelForCausalLM
# import streamlit as st
# 
# # 设置清华镜像源（适用于中国大陆用户）
# os.environ['HF_HOME'] = '/content/transformers_cache'  # 设置 Hugging Face 缓存目录
# os.environ['TRANSFORMERS_CACHE'] = '/content/transformers_cache'  # 设置 transformers 缓存目录
# os.environ['HF_HUB_OFFLINE'] = '1'  # 开启离线模式，避免每次都下载
# 
# # 设置 GPU 设备
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# print(f"Using device: {device}")  # 输出当前使用的设备（GPU 或 CPU）
# 
# # 加载模型和tokenizer
# def load_model():
#     tokenizer = AutoTokenizer.from_pretrained("beita6969/deepseek-r1-medical-response")
#     model = AutoModelForCausalLM.from_pretrained("beita6969/deepseek-r1-medical-response").to(device)  # 将模型加载到 GPU
#     return tokenizer, model
# 
# tokenizer, model = load_model()
# 
# def generate_answer(question):
#     """
#     使用模型生成问题的回答
#     """
#     try:
#         # 对问题进行编码
#         inputs = tokenizer(question, return_tensors="pt").to(device)  # 将输入数据转移到 GPU
# 
#         # 生成模型的输出
#         with torch.no_grad():
#             outputs = model.generate(
#                 inputs["input_ids"],
#                 max_length=300,  # 设置最大回复长度
#                 num_return_sequences=1,
#                 no_repeat_ngram_size=2,
#                 top_p=0.95,
#                 top_k=50,
#                 temperature=0.7,
#                 pad_token_id=tokenizer.eos_token_id,
#             )
# 
#         # 解码模型生成的结果
#         answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
# 
#         # 后处理：如果生成的回答中包含问题，可以去除
#         answer = answer.replace(question, "").strip()  # 去除重复的问题部分
# 
#         # 确保回答内容完整并且精简
#         if len(answer) < 20:  # 如果生成的回答太短，尝试重新生成
#             answer = "抱歉，我没有得到足够的信息来回答您的问题，请再试一次。"
# 
#         return answer
#     except Exception as e:
#         st.error(f"发生错误: {str(e)}")
#         return "抱歉，无法生成回答。"
# 
# def main():
#     # 历史记录
#     if "history" not in st.session_state:
#         st.session_state.history = []
# 
#     # 自定义 CSS 样式
#     st.markdown(
#         """
#         <style>
#         .main {
#             background-color: #f0f2f6;
#             padding: 30px;
#             border-radius: 10px;
#             box-shadow: 0 4px 8px rgba(0,0,0,0.1);
#         }
#         .title {
#             font-size: 36px;
#             color: #2c3e50;
#             text-align: center;
#             margin-bottom: 10px;
#         }
#         .subheader {
#             font-size: 20px;
#             color: #34495e;
#             text-align: center;
#             margin-bottom: 20px;
#         }
#         .history {
#             margin-top: 20px;
#             padding-top: 10px;
#             border-top: 2px solid #ddd;
#         }
#         .user-query {
#             background-color: #ecf0f1;
#             padding: 8px;
#             border-radius: 10px;
#         }
#         .model-response {
#             background-color: #d5f5e3;
#             padding: 8px;
#             border-radius: 10px;
#         }
#         </style>
#         """, unsafe_allow_html=True
#     )
# 
#     # 主体内容
#     st.markdown('<div class="main">', unsafe_allow_html=True)
#     st.markdown('<h1 class="title">医学问答系统</h1>', unsafe_allow_html=True)
#     st.markdown('<p class="subheader">请输入您的医学问题，我们将为您提供参考答案</p>', unsafe_allow_html=True)
# 
#     # 显示历史记录
#     if st.session_state.history:
#         for idx, (query, response) in enumerate(st.session_state.history):
#             st.markdown(f"**问题 {idx+1}:**")
#             st.markdown(f'<div class="user-query">{query}</div>', unsafe_allow_html=True)
#             st.markdown(f"**回答 {idx+1}:**")
#             st.markdown(f'<div class="model-response">{response}</div>', unsafe_allow_html=True)
# 
#     # 用户输入区域
#     question = st.text_area("您的问题：", height=120, placeholder="例如：我最近发烧，该怎么办？")
# 
#     # 提交按钮
#     if st.button("提交问题"):
#         if not question.strip():
#             st.error("请输入问题后再提交。")
#         else:
#             with st.spinner("正在获取答案..."):
#                 answer = generate_answer(question)
#                 # 将问题和回答添加到历史记录
#                 st.session_state.history.append((question, answer))
# 
#             # 显示最新的回答
#             st.success("回答如下：")
#             st.write(answer)
# 
#     st.markdown('</div>', unsafe_allow_html=True)
# 
# if __name__ == '__main__':
#     main()

from pyngrok import ngrok

# 设置您的 ngrok 认证令牌
ngrok.set_auth_token("YOUR_NGROK_AUTH_TOKEN")  # 将 "YOUR_NGROK_AUTH_TOKEN" 替换为您从 ngrok 获取的 authtoken

# 设置端口并启动 ngrok 隧道
port = 8501  # 默认的 Streamlit 端口
public_url = ngrok.connect(port)
print(f"Streamlit 应用已通过以下 URL 公开: {public_url}")

!pip install bitsandbytes

!curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh

!pip install transformers

!pip install triton==2.0.0

# 安装最新版本的 tokenizers
!pip install --upgrade tokenizers

!pip install --no-cache-dir transformers

!streamlit run /content/medical_qa_app.py

!nvcc --version

!pip install torch torchvision torchaudio --upgrade
!pip install bitsandbytes-cuda113

!nvcc --version

!pip install bitsandbytes-cuda113

!pip install bitsandbytes

!pip install accelerate

!pip uninstall -y bitsandbytes
!pip install bitsandbytes-cuda112

import torch
torch.cuda.empty_cache()

!pip install bitsandbytes

# 安装 bitsandbytes 和 triton 的兼容版本
!pip install bitsandbytes==0.39.0 triton==2.0.0

# 卸载现有版本的 torch 和 torchvision
!pip uninstall torch torchvision -y

# 安装与 torch==2.0.1 兼容的 torchvision==0.15.2
!pip install torch==2.0.1 torchvision==0.15.2