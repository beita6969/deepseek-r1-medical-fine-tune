{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit torch transformers"
      ],
      "metadata": {
        "collapsed": true,
        "id": "PdR1XadUffQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "id": "wIcvyPRNfqac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile medical_qa_app.py\n",
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import streamlit as st\n",
        "\n",
        "# 设置清华镜像源（适用于中国大陆用户）\n",
        "os.environ['HF_HOME'] = '/content/transformers_cache'  # 设置 Hugging Face 缓存目录\n",
        "os.environ['TRANSFORMERS_CACHE'] = '/content/transformers_cache'  # 设置 transformers 缓存目录\n",
        "os.environ['HF_HUB_OFFLINE'] = '1'  # 开启离线模式，避免每次都下载\n",
        "\n",
        "# 设置 GPU 设备\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")  # 输出当前使用的设备（GPU 或 CPU）\n",
        "\n",
        "# 加载模型和tokenizer\n",
        "def load_model():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"beita6969/deepseek-r1-medical-response\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"beita6969/deepseek-r1-medical-response\").to(device)  # 将模型加载到 GPU\n",
        "    return tokenizer, model\n",
        "\n",
        "tokenizer, model = load_model()\n",
        "\n",
        "def generate_answer(question):\n",
        "    \"\"\"\n",
        "    使用模型生成问题的回答\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 对问题进行编码\n",
        "        inputs = tokenizer(question, return_tensors=\"pt\").to(device)  # 将输入数据转移到 GPU\n",
        "\n",
        "        # 生成模型的输出\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                inputs[\"input_ids\"],\n",
        "                max_length=300,  # 设置最大回复长度\n",
        "                num_return_sequences=1,\n",
        "                no_repeat_ngram_size=2,\n",
        "                top_p=0.95,\n",
        "                top_k=50,\n",
        "                temperature=0.7,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "            )\n",
        "\n",
        "        # 解码模型生成的结果\n",
        "        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # 后处理：如果生成的回答中包含问题，可以去除\n",
        "        answer = answer.replace(question, \"\").strip()  # 去除重复的问题部分\n",
        "\n",
        "        # 确保回答内容完整并且精简\n",
        "        if len(answer) < 20:  # 如果生成的回答太短，尝试重新生成\n",
        "            answer = \"抱歉，我没有得到足够的信息来回答您的问题，请再试一次。\"\n",
        "\n",
        "        return answer\n",
        "    except Exception as e:\n",
        "        st.error(f\"发生错误: {str(e)}\")\n",
        "        return \"抱歉，无法生成回答。\"\n",
        "\n",
        "def main():\n",
        "    # 历史记录\n",
        "    if \"history\" not in st.session_state:\n",
        "        st.session_state.history = []\n",
        "\n",
        "    # 自定义 CSS 样式\n",
        "    st.markdown(\n",
        "        \"\"\"\n",
        "        <style>\n",
        "        .main {\n",
        "            background-color: #f0f2f6;\n",
        "            padding: 30px;\n",
        "            border-radius: 10px;\n",
        "            box-shadow: 0 4px 8px rgba(0,0,0,0.1);\n",
        "        }\n",
        "        .title {\n",
        "            font-size: 36px;\n",
        "            color: #2c3e50;\n",
        "            text-align: center;\n",
        "            margin-bottom: 10px;\n",
        "        }\n",
        "        .subheader {\n",
        "            font-size: 20px;\n",
        "            color: #34495e;\n",
        "            text-align: center;\n",
        "            margin-bottom: 20px;\n",
        "        }\n",
        "        .history {\n",
        "            margin-top: 20px;\n",
        "            padding-top: 10px;\n",
        "            border-top: 2px solid #ddd;\n",
        "        }\n",
        "        .user-query {\n",
        "            background-color: #ecf0f1;\n",
        "            padding: 8px;\n",
        "            border-radius: 10px;\n",
        "        }\n",
        "        .model-response {\n",
        "            background-color: #d5f5e3;\n",
        "            padding: 8px;\n",
        "            border-radius: 10px;\n",
        "        }\n",
        "        </style>\n",
        "        \"\"\", unsafe_allow_html=True\n",
        "    )\n",
        "\n",
        "    # 主体内容\n",
        "    st.markdown('<div class=\"main\">', unsafe_allow_html=True)\n",
        "    st.markdown('<h1 class=\"title\">医学问答系统</h1>', unsafe_allow_html=True)\n",
        "    st.markdown('<p class=\"subheader\">请输入您的医学问题，我们将为您提供参考答案</p>', unsafe_allow_html=True)\n",
        "\n",
        "    # 显示历史记录\n",
        "    if st.session_state.history:\n",
        "        for idx, (query, response) in enumerate(st.session_state.history):\n",
        "            st.markdown(f\"**问题 {idx+1}:**\")\n",
        "            st.markdown(f'<div class=\"user-query\">{query}</div>', unsafe_allow_html=True)\n",
        "            st.markdown(f\"**回答 {idx+1}:**\")\n",
        "            st.markdown(f'<div class=\"model-response\">{response}</div>', unsafe_allow_html=True)\n",
        "\n",
        "    # 用户输入区域\n",
        "    question = st.text_area(\"您的问题：\", height=120, placeholder=\"例如：我最近发烧，该怎么办？\")\n",
        "\n",
        "    # 提交按钮\n",
        "    if st.button(\"提交问题\"):\n",
        "        if not question.strip():\n",
        "            st.error(\"请输入问题后再提交。\")\n",
        "        else:\n",
        "            with st.spinner(\"正在获取答案...\"):\n",
        "                answer = generate_answer(question)\n",
        "                # 将问题和回答添加到历史记录\n",
        "                st.session_state.history.append((question, answer))\n",
        "\n",
        "            # 显示最新的回答\n",
        "            st.success(\"回答如下：\")\n",
        "            st.write(answer)\n",
        "\n",
        "    st.markdown('</div>', unsafe_allow_html=True)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "NBSMRKNPfuvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# 设置您的 ngrok 认证令牌\n",
        "ngrok.set_auth_token(\"YOUR_NGROK_AUTH_TOKEN\")  # 将 \"YOUR_NGROK_AUTH_TOKEN\" 替换为您从 ngrok 获取的 authtoken\n",
        "\n",
        "# 设置端口并启动 ngrok 隧道\n",
        "port = 8501  # 默认的 Streamlit 端口\n",
        "public_url = ngrok.connect(port)\n",
        "print(f\"Streamlit 应用已通过以下 URL 公开: {public_url}\")"
      ],
      "metadata": {
        "id": "vjIsVtjMfzZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ALKcDiWOoRf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh"
      ],
      "metadata": {
        "collapsed": true,
        "id": "cNHShmC2yEOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "collapsed": true,
        "id": "x5uk5DOtyK_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install triton==2.0.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7ZMPsCzJw9_7",
        "outputId": "a6ec749a-9cdf-4b77-979b-e8e4040c197f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.11/dist-packages (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0) (3.31.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0) (3.17.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0) (2.0.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0) (18.1.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (11.7.91)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->triton==2.0.0) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->triton==2.0.0) (0.45.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->triton==2.0.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch->triton==2.0.0) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 安装最新版本的 tokenizers\n",
        "!pip install --upgrade tokenizers"
      ],
      "metadata": {
        "collapsed": true,
        "id": "8zka-_ysxF1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --no-cache-dir transformers"
      ],
      "metadata": {
        "collapsed": true,
        "id": "q_DDxX_XxJrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /content/medical_qa_app.py"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KOWG--tdhNr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "id": "3-dmGDIGD45Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio --upgrade\n",
        "!pip install bitsandbytes-cuda113"
      ],
      "metadata": {
        "id": "V8F5nkoaDChS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "id": "2mrnioy2C714"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes-cuda113"
      ],
      "metadata": {
        "id": "dp066kHvC7uT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "id": "ATu7nblLAF7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "eHIyAQB8Abep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y bitsandbytes\n",
        "!pip install bitsandbytes-cuda112"
      ],
      "metadata": {
        "id": "rHYeKIG1-yNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "LLEURrOn-2Vo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "id": "eiBAMG9rzGTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 安装 bitsandbytes 和 triton 的兼容版本\n",
        "!pip install bitsandbytes==0.39.0 triton==2.0.0"
      ],
      "metadata": {
        "id": "gw1EVOzQwBjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 卸载现有版本的 torch 和 torchvision\n",
        "!pip uninstall torch torchvision -y\n",
        "\n",
        "# 安装与 torch==2.0.1 兼容的 torchvision==0.15.2\n",
        "!pip install torch==2.0.1 torchvision==0.15.2"
      ],
      "metadata": {
        "id": "uJKBs9A4sj1i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}